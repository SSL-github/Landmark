{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/smart02/Yonsei_Dataset'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "from multiprocessing import Process, Pool, Lock, Queue\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.python.platform import app\n",
    "from delf import aggregation_config_pb2\n",
    "from delf import datum_io\n",
    "from delf import feature_aggregation_similarity\n",
    "from delf.python.detect_to_retrieve import dataset\n",
    "from delf.python.detect_to_retrieve import image_reranking\n",
    "from delf import feature_io\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PR_RANKS = (1, 5, 10)\n",
    "\n",
    "_STATUS_CHECK_LOAD_ITERATIONS = 50\n",
    "\n",
    "_METRICS_FILENAME = 'metrics.txt'\n",
    "\n",
    "@jit(nopython = True)\n",
    "def euclidean_distance_numba(x, y):\n",
    "    return np.sqrt(np.sum((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT 위치\n",
    "query_list, index_list, ground_truth, _, _ = dataset.ReadDatasetFile('/home/smart02/Yonsei_Dataset/Yonsei_Dataset_for_Feature/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_ = [i.replace(\"JPG\",'npy') for i in index_list]\n",
    "query_list_ = [i.replace(\"JPG\",'npy') for i in query_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103701\n"
     ]
    }
   ],
   "source": [
    "# query 개수 확인!\n",
    "print(len(query_list))\n",
    "# query_list\n",
    "#query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103701\n",
      "404229\n"
     ]
    }
   ],
   "source": [
    "# query 개수, index 개수를 변수로 지정\n",
    "# query 개수가 변동되는 경우, 여기를 변경.\n",
    "num_query_images = len(query_list)\n",
    "num_index_images = len(index_list)\n",
    "print(num_query_images)\n",
    "print(num_index_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뽑혀진 query feature와 index feature를 pickle로 저장한 위치를 지정.\n",
    "query_descriptors_ = pkl.load(open('/home/smart02/LM/research/delf/delf/python/training/final_query_features_0126.txt', 'rb'))\n",
    "index_descriptors = pkl.load(open(\"/home/smart02/LM/research/delf/delf/python/training/final_index_features_0126_1.txt\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 개수에 변동이 생기는 경우, 여기서 handling 하면 됨.\n",
    "query_descriptors = query_descriptors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_retrieval(query_index):\n",
    "    sample_query = query_descriptors[query_index]\n",
    "    similarities = np.zeros([num_index_images])\n",
    "    for index_idx, descriptors in enumerate(index_descriptors):\n",
    "        similarities[index_idx] = euclidean_distance_numba(sample_query, descriptors)\n",
    "    return np.argsort(similarities)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103701/103701 [3:01:47<00:00,  9.51it/s] \n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "query_index = [i for i in range(len(query_descriptors))]\n",
    "with Pool(20) as p:\n",
    "    res = list(tqdm(p.imap(multi_retrieval, query_index), total=len(query_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103701, 1000)\n"
     ]
    }
   ],
   "source": [
    "# res를 numpy array로 변경해서 result로 지정.\n",
    "result = np.asarray(res)\n",
    "\n",
    "# result shape 확인, (query 개수, 1000) 으로 찍히면 Ok.\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result를 .npy로 저장\n",
    "np.save('/home/smart02/Yonsei_Dataset/rank_result', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103701, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (24, 1000) 나오면 정상!\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PR_RANKS = (1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAveragePrecision(positive_ranks):\n",
    "  \"\"\"Computes average precision according to dataset convention.\n",
    "\n",
    "  It assumes that `positive_ranks` contains the ranks for all expected positive\n",
    "  index images to be retrieved. If `positive_ranks` is empty, returns\n",
    "  `average_precision` = 0.\n",
    "\n",
    "  Note that average precision computation here does NOT use the finite sum\n",
    "  method (see\n",
    "  https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision)\n",
    "  which is common in information retrieval literature. Instead, the method\n",
    "  implemented here integrates over the precision-recall curve by averaging two\n",
    "  adjacent precision points, then multiplying by the recall step. This is the\n",
    "  convention for the Revisited Oxford/Paris datasets.\n",
    "\n",
    "  Args:\n",
    "    positive_ranks: Sorted 1D NumPy integer array, zero-indexed.\n",
    "\n",
    "  Returns:\n",
    "    average_precision: Float.\n",
    "  \"\"\"\n",
    "  average_precision = 0.0\n",
    "\n",
    "  num_expected_positives = len(positive_ranks)\n",
    "  if not num_expected_positives:\n",
    "    return average_precision\n",
    "\n",
    "  recall_step = 1.0 / num_expected_positives\n",
    "  for i, rank in enumerate(positive_ranks):\n",
    "    if not rank:\n",
    "      left_precision = 1.0\n",
    "    else:\n",
    "      left_precision = i / rank\n",
    "\n",
    "    right_precision = (i + 1) / (rank + 1)\n",
    "    average_precision += (left_precision + right_precision) * recall_step / 2\n",
    "\n",
    "  return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMetrics(sorted_index_ids, ground_truth, query_list, desired_pr_ranks, check, start=0):\n",
    "  \"\"\"Computes metrics for retrieval results on the Revisited datasets.\n",
    "\n",
    "  If there are no valid ground-truth index images for a given query, the metric\n",
    "  results for the given query (`average_precisions`, `precisions` and `recalls`)\n",
    "  are set to NaN, and they are not taken into account when computing the\n",
    "  aggregated metrics (`mean_average_precision`, `mean_precisions` and\n",
    "  `mean_recalls`) over all queries.\n",
    "\n",
    "  Args:\n",
    "    sorted_index_ids: Integer NumPy array of shape [#queries, #index_images].\n",
    "      For each query, contains an array denoting the most relevant index images,\n",
    "      sorted from most to least relevant.\n",
    "    ground_truth: List containing ground-truth information for dataset. Each\n",
    "      entry is a dict corresponding to the ground-truth information for a query.\n",
    "      The dict has keys 'ok' and 'junk', mapping to a NumPy array of integers.\n",
    "    desired_pr_ranks: List of integers containing the desired precision/recall\n",
    "      ranks to be reported. Eg, if precision@1/recall@1 and\n",
    "      precision@10/recall@10 are desired, this should be set to [1, 10]. The\n",
    "      largest item should be <= #index_images.\n",
    "\n",
    "  Returns:\n",
    "    mean_average_precision: Mean average precision (float).\n",
    "    mean_precisions: Mean precision @ `desired_pr_ranks` (NumPy array of\n",
    "      floats, with shape [len(desired_pr_ranks)]).\n",
    "    mean_recalls: Mean recall @ `desired_pr_ranks` (NumPy array of floats, with\n",
    "      shape [len(desired_pr_ranks)]).\n",
    "    average_precisions: Average precision for each query (NumPy array of floats,\n",
    "      with shape [#queries]).\n",
    "    precisions: Precision @ `desired_pr_ranks`, for each query (NumPy array of\n",
    "      floats, with shape [#queries, len(desired_pr_ranks)]).\n",
    "    recalls: Recall @ `desired_pr_ranks`, for each query (NumPy array of\n",
    "      floats, with shape [#queries, len(desired_pr_ranks)]).\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If largest desired PR rank in `desired_pr_ranks` >\n",
    "      #index_images.\n",
    "  \"\"\"\n",
    "  sorted_index_ids = sorted_index_ids[:,:check]\n",
    "  num_queries, num_index_images = sorted_index_ids.shape\n",
    "  num_desired_pr_ranks = len(desired_pr_ranks)\n",
    "  ground_truth = ground_truth[start:]\n",
    "  query_list =  query_list[start:]\n",
    "  sorted_desired_pr_ranks = sorted(desired_pr_ranks)\n",
    "\n",
    "  if sorted_desired_pr_ranks[-1] > num_index_images:\n",
    "    raise ValueError(\n",
    "        'Requested PR ranks up to %d, however there are only %d images' %\n",
    "        (sorted_desired_pr_ranks[-1], num_index_images))\n",
    "\n",
    "  # Instantiate all outputs, then loop over each query and gather metrics.\n",
    "  mean_average_precision = 0.0\n",
    "#   mean_precisions = np.zeros([num_desired_pr_ranks])\n",
    "#   mean_recalls = np.zeros([num_desired_pr_ranks])\n",
    "  average_precisions = np.zeros([num_queries])\n",
    "#   precisions = np.zeros([num_queries, num_desired_pr_ranks])\n",
    "#   recalls = np.zeros([num_queries, num_desired_pr_ranks])\n",
    "  num_empty_gt_queries = 0\n",
    "  for i, im in zip(range(num_queries), query_list):\n",
    "#     ok_index_images = ground_truth[i]['ok']\n",
    "#     junk_index_images = ground_truth[i]['junk']\n",
    "    ok_index_images = ground_truth[i][im]\n",
    "#   for i in range(num_queries):\n",
    "#     ok_index_images = ground_truth[i]['ok']\n",
    "#     junk_index_images = ground_truth[i]['junk']\n",
    "\n",
    "    if not ok_index_images.size:\n",
    "      average_precisions[i] = float('nan')\n",
    "#       precisions[i, :] = float('nan')\n",
    "#       recalls[i, :] = float('nan')\n",
    "      num_empty_gt_queries += 1\n",
    "      continue\n",
    "    \n",
    "    positive_ranks = np.arange(num_index_images)[np.in1d(\n",
    "        sorted_index_ids[i], ok_index_images)]\n",
    "#     junk_ranks = np.arange(num_index_images)[np.in1d(sorted_index_ids[i],\n",
    "#                                                      junk_index_images)]\n",
    "    adjusted_positive_ranks = AdjustPositiveRanks(positive_ranks)\n",
    "#     adjusted_positive_ranks = AdjustPositiveRanks(positive_ranks, junk_ranks)\n",
    "\n",
    "\n",
    "    average_precisions[i] = ComputeAveragePrecision(adjusted_positive_ranks)\n",
    "#     precisions[i, :], recalls[i, :] = ComputePRAtRanks(adjusted_positive_ranks,\n",
    "#                                                        desired_pr_ranks)\n",
    "#     np.save('./precisions.npy',precisions)\n",
    "#    print(i, average_precisions[i])\n",
    "    mean_average_precision += average_precisions[i]\n",
    "#     mean_precisions += precisions[i, :]\n",
    "#     mean_recalls += recalls[i, :]\n",
    "\n",
    "#   Normalize aggregated metrics by number of queries.\n",
    "  num_valid_queries = num_queries - num_empty_gt_queries\n",
    "  mean_average_precision /= num_valid_queries\n",
    "#   mean_precisions /= num_valid_queries\n",
    "#   mean_recalls /= num_valid_queries\n",
    "\n",
    "  return mean_average_precision*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustPositiveRanks(positive_ranks):\n",
    "  \"\"\"Adjusts positive ranks based on junk ranks.\n",
    "\n",
    "  Args:\n",
    "    positive_ranks: Sorted 1D NumPy integer array.\n",
    "    junk_ranks: Sorted 1D NumPy integer array.\n",
    "\n",
    "  Returns:\n",
    "    adjusted_positive_ranks: Sorted 1D NumPy array.\n",
    "  \"\"\"\n",
    "#   if not junk_ranks.size:\n",
    "#     return positive_ranks\n",
    "\n",
    "  adjusted_positive_ranks = positive_ranks\n",
    "#   j = 0\n",
    "#   for i, positive_index in enumerate(positive_ranks):\n",
    "#     while (j < len(junk_ranks) and positive_index > junk_ranks[j]):\n",
    "#       j += 1\n",
    "\n",
    "#     adjusted_positive_ranks[i] -= j\n",
    "\n",
    "  return adjusted_positive_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP 99.79396062368828\n",
      "mAP 99.64722357813831\n",
      "mAP 99.47960469503178\n",
      "mAP 98.17796167487623\n",
      "mAP 95.78018641683725\n"
     ]
    }
   ],
   "source": [
    "# mAP 계산, mAP_start는 시작하는 지점을 의미, 0으로 지정하면 됨.\n",
    "\n",
    "mAP_start = 0\n",
    "#print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 5, start = mAP_start))\n",
    "print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 10, start = mAP_start))\n",
    "print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 15, start = mAP_start))\n",
    "print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 20, start = mAP_start))\n",
    "print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 50, start = mAP_start))\n",
    "print('mAP', ComputeMetrics(result, ground_truth, query_list, _PR_RANKS, 100, start = mAP_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(new_t)",
   "language": "python",
   "name": "new_t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
